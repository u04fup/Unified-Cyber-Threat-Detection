{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977580e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score, f1_score, recall_score, confusion_matrix\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# USER CONFIGURATION\n",
    "# ==========================================\n",
    "# Please update these paths before running\n",
    "PATHS = {\n",
    "    \"T1_TRAIN\": \"/path/to/train_phish_email_list.csv\",\n",
    "    \"T1_TEST\":  \"/path/to/test_phish_email_list.csv\",\n",
    "    \"T2_TRAIN\": \"/path/to/train_malicious_phish.csv\",\n",
    "    \"T2_TEST\":  \"/path/to/test_malicious_phish.csv\",\n",
    "    \"T3_TRAIN\": \"/path/to/train_malicious_phish_multi.csv\",\n",
    "    \"T3_TEST\":  \"/path/to/test_malicious_phish_multi.csv\"\n",
    "}\n",
    "\n",
    "SEEDS = [0, 1, 2]\n",
    "\n",
    "# ==========================================\n",
    "# UTILITY FUNCTIONS\n",
    "# ==========================================\n",
    "def print_latex_stats(results_dict, metric_names):\n",
    "    \"\"\"Calculates Mean +/- Std and prints in LaTeX format.\"\"\"\n",
    "    print(\"-\" * 60)\n",
    "    print(\"=== Final Aggregated Results (LaTeX Format) ===\")\n",
    "    df_res = pd.DataFrame(results_dict)\n",
    "    \n",
    "    for col in metric_names:\n",
    "        mean_val = df_res[col].mean()\n",
    "        std_val = df_res[col].std()\n",
    "        print(f\"{col:<15}: ${mean_val:.4f} \\\\pm {std_val:.4f}$\")\n",
    "    print(\"-\" * 60 + \"\\n\")\n",
    "\n",
    "# ==========================================\n",
    "# TASK T1: Phishing Email (Binary)\n",
    "# ==========================================\n",
    "def run_task_t1():\n",
    "    print(f\"\\n{'='*20} STARTING TASK T1 (Phishing Email) {'='*20}\")\n",
    "    \n",
    "    # 1. Load Data\n",
    "    print(\"Loading T1 data...\")\n",
    "    df_train = pd.read_csv(PATHS[\"T1_TRAIN\"])\n",
    "    df_test = pd.read_csv(PATHS[\"T1_TEST\"])\n",
    "\n",
    "    df_train['data'].fillna('', inplace=True)\n",
    "    df_test['data'].fillna('', inplace=True)\n",
    "    \n",
    "    X_train_raw = df_train['data']\n",
    "    y_train = df_train['label']\n",
    "    X_test_raw = df_test['data']\n",
    "    y_test = df_test['label'].values\n",
    "\n",
    "    # 2. Vectorization (Word-level)\n",
    "    print(\"Vectorizing (Word TF-IDF)...\")\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=15000, stop_words='english', \n",
    "        ngram_range=(1, 2), sublinear_tf=True\n",
    "    )\n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train_raw)\n",
    "    X_test_tfidf = vectorizer.transform(X_test_raw)\n",
    "\n",
    "    # 3. Training Loop\n",
    "    history = {'AUPRC': [], 'F1-score': [], 'Recall': [], 'Latency (ms)': []}\n",
    "\n",
    "    for seed in SEEDS:\n",
    "        print(f\"Training Seed {seed} (Subsampling 80%)...\")\n",
    "        # Subsample training data to introduce variance\n",
    "        X_sub, _, y_sub, _ = train_test_split(\n",
    "            X_train_tfidf, y_train, train_size=0.8, random_state=seed, stratify=y_train\n",
    "        )\n",
    "\n",
    "        model = LogisticRegression(solver='liblinear', random_state=seed, max_iter=1000)\n",
    "        model.fit(X_sub, y_sub)\n",
    "        t_start = time.time()\n",
    "        y_prob = model.predict_proba(X_test_tfidf)[:, 1]\n",
    "        y_pred = (y_prob >= 0.5).astype(np.int32)\n",
    "        t_end = time.time()\n",
    "        \n",
    "        total_infer_time = t_end - t_start\n",
    "        latency_ms_per_sample = (total_infer_time * 1000) / X_test_tfidf.shape[0]\n",
    "        history['Latency (ms)'].append(latency_ms_per_sample)\n",
    "\n",
    "        auprc = average_precision_score(y_test, y_prob)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        rec = recall_score(y_test, y_pred)\n",
    "\n",
    "        history['AUPRC'].append(auprc)\n",
    "        history['F1-score'].append(f1)\n",
    "        history['Recall'].append(rec)\n",
    "\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        print(f\"  CM: {cm.tolist()}\")\n",
    "        print(f\"  Latency: {latency_ms_per_sample:.4f} ms/sample\")\n",
    "        print(f\"  [Seed {seed}] AUPRC: {auprc:.4f} | F1-score: {f1:.4f} | Recall: {rec:.4f}\")\n",
    "\n",
    "    print_latex_stats(history, ['AUPRC', 'F1-score', 'Recall'])\n",
    "\n",
    "# ==========================================\n",
    "# TASK T2: Malicious URL (Binary)\n",
    "# ==========================================\n",
    "def run_task_t2():\n",
    "    print(f\"\\n{'='*20} STARTING TASK T2 (Malicious URL - Binary) {'='*20}\")\n",
    "\n",
    "    # 1. Load Data\n",
    "    print(\"Loading T2 data...\")\n",
    "    df_train = pd.read_csv(PATHS[\"T2_TRAIN\"])\n",
    "    df_test = pd.read_csv(PATHS[\"T2_TEST\"])\n",
    "\n",
    "    df_train['url'].fillna('', inplace=True)\n",
    "    df_test['url'].fillna('', inplace=True)\n",
    "\n",
    "    X_train_raw = df_train['url']\n",
    "    y_train = df_train['label']\n",
    "    X_test_raw = df_test['url']\n",
    "    y_test = df_test['label'].values\n",
    "\n",
    "    # 2. Vectorization (Char-level)\n",
    "    print(\"Vectorizing (Char TF-IDF)...\")\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        analyzer='char', ngram_range=(3, 7), \n",
    "        max_features=30000, min_df=3\n",
    "    )\n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train_raw)\n",
    "    X_test_tfidf = vectorizer.transform(X_test_raw)\n",
    "\n",
    "    # 3. Training Loop\n",
    "    history = {'AUPRC': [], 'F1-score': [], 'Recall': [], 'Latency (ms)': []}\n",
    "\n",
    "    for seed in SEEDS:\n",
    "        print(f\"Training Seed {seed} (Subsampling 80%)...\")\n",
    "        X_sub, _, y_sub, _ = train_test_split(\n",
    "            X_train_tfidf, y_train, train_size=0.8, random_state=seed, stratify=y_train\n",
    "        )\n",
    "\n",
    "        model = LogisticRegression(solver='liblinear', random_state=seed, max_iter=1000)\n",
    "        model.fit(X_sub, y_sub)\n",
    "        t_start = time.time()\n",
    "        y_prob = model.predict_proba(X_test_tfidf)[:, 1]\n",
    "        y_pred = (y_prob >= 0.5).astype(np.int32)\n",
    "        t_end = time.time()\n",
    "\n",
    "        total_infer_time = t_end - t_start\n",
    "        latency_ms_per_sample = (total_infer_time * 1000) / X_test_tfidf.shape[0]\n",
    "        history['Latency (ms)'].append(latency_ms_per_sample)\n",
    "\n",
    "        auprc = average_precision_score(y_test, y_prob)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        rec = recall_score(y_test, y_pred)\n",
    "\n",
    "        history['AUPRC'].append(auprc)\n",
    "        history['F1-score'].append(f1)\n",
    "        history['Recall'].append(rec)\n",
    "        \n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        print(f\"  CM: {cm.tolist()}\")\n",
    "        print(f\"  Latency: {latency_ms_per_sample:.4f} ms/sample\")\n",
    "        print(f\"  [Seed {seed}] AUPRC: {auprc:.4f} | F1-score: {f1:.4f} | Recall: {rec:.4f}\")\n",
    "\n",
    "    print_latex_stats(history, ['AUPRC', 'F1-score', 'Recall'])\n",
    "\n",
    "# ==========================================\n",
    "# TASK T3: Malicious URL (Multiclass)\n",
    "# ==========================================\n",
    "def run_task_t3():\n",
    "    print(f\"\\n{'='*20} STARTING TASK T3 (Malicious URL - Multiclass) {'='*20}\")\n",
    "\n",
    "    # 1. Load Data\n",
    "    print(\"Loading T3 data...\")\n",
    "    df_train = pd.read_csv(PATHS[\"T3_TRAIN\"])\n",
    "    df_test = pd.read_csv(PATHS[\"T3_TEST\"])\n",
    "\n",
    "    df_train['url'].fillna('', inplace=True)\n",
    "    df_test['url'].fillna('', inplace=True)\n",
    "\n",
    "    X_train_raw = df_train['url']\n",
    "    y_train = df_train['label']\n",
    "    X_test_raw = df_test['url']\n",
    "    y_test = df_test['label'].values\n",
    "\n",
    "    # 2. Vectorization (Char-level)\n",
    "    print(\"Vectorizing (Char TF-IDF)...\")\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        analyzer='char', ngram_range=(3, 7), \n",
    "        max_features=30000, min_df=3\n",
    "    )\n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train_raw)\n",
    "    X_test_tfidf = vectorizer.transform(X_test_raw)\n",
    "\n",
    "    # Prepare labels for Macro AUPRC\n",
    "    lb = LabelBinarizer()\n",
    "    y_test_bin = lb.fit_transform(y_test)\n",
    "    \n",
    "    if len(lb.classes_) == 2:\n",
    "        y_test_bin = np.hstack((1 - y_test_bin, y_test_bin))\n",
    "\n",
    "    # 3. Training Loop\n",
    "    history = {'Macro AUPRC': [], 'Macro F1-score': [], 'Macro Recall': [], 'Latency (ms)': []}\n",
    "\n",
    "    for seed in SEEDS:\n",
    "        print(f\"Training Seed {seed} (Subsampling 80%)...\")\n",
    "        X_sub, _, y_sub, _ = train_test_split(\n",
    "            X_train_tfidf, y_train, train_size=0.8, random_state=seed, stratify=y_train\n",
    "        )\n",
    "\n",
    "        model = LogisticRegression(solver='liblinear', random_state=seed, max_iter=1000)\n",
    "        model.fit(X_sub, y_sub)\n",
    "        t_start = time.time()\n",
    "        y_pred = model.predict(X_test_tfidf)\n",
    "        y_prob = model.predict_proba(X_test_tfidf)\n",
    "        t_end = time.time()\n",
    "\n",
    "        total_infer_time = t_end - t_start\n",
    "        latency_ms_per_sample = (total_infer_time * 1000) / X_test_tfidf.shape[0]\n",
    "        history['Latency (ms)'].append(latency_ms_per_sample)\n",
    "\n",
    "        auprc = average_precision_score(y_test_bin, y_prob, average='macro')\n",
    "        f1 = f1_score(y_test, y_pred, average='macro')\n",
    "        rec = recall_score(y_test, y_pred, average='macro')\n",
    "\n",
    "        history['Macro AUPRC'].append(auprc)\n",
    "        history['Macro F1-score'].append(f1)\n",
    "        history['Macro Recall'].append(rec)\n",
    "        \n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        print(f\"  CM: {cm.tolist()}\")\n",
    "        print(f\"  Latency: {latency_ms_per_sample:.4f} ms/sample\")\n",
    "        print(f\"  [Seed {seed}] AUPRC: {auprc:.4f} | F1-score: {f1:.4f} | Recall: {rec:.4f}\")\n",
    "\n",
    "    print_latex_stats(history, ['Macro AUPRC', 'Macro F1-score', 'Macro Recall'])\n",
    "\n",
    "# ==========================================\n",
    "# MAIN EXECUTION\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        run_task_t1()\n",
    "        run_task_t2()\n",
    "        run_task_t3()\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\n[ERROR] File not found. Check 'PATHS' configuration.\\n{e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[ERROR] An error occurred:\\n{e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8dd094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, label_binarize\n",
    "from sklearn.metrics import (\n",
    "    recall_score, f1_score, confusion_matrix, average_precision_score\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# USER CONFIGURATION\n",
    "# ==========================================\n",
    "# Please update these paths before running\n",
    "PATHS = {\n",
    "    \"T4_TRAIN\": \"/path/to/train_creditcard_timesplit.csv\",\n",
    "    \"T4_TEST\":  \"/path/to/test_creditcard_timesplit.csv\",\n",
    "    \"UNSW_TRAIN\": \"/path/to/UNSW_NB15_training-set.parquet\",\n",
    "    \"UNSW_TEST\":  \"/path/to/UNSW_NB15_testing-set.parquet\"\n",
    "}\n",
    "\n",
    "SEEDS = [0, 1, 2]\n",
    "\n",
    "# ==========================================\n",
    "# UTILITY FUNCTIONS\n",
    "# ==========================================\n",
    "def print_latex_stats(results_dict, metric_names):\n",
    "    \"\"\"Calculates Mean +/- Std and prints in LaTeX format.\"\"\"\n",
    "    print(\"-\" * 60)\n",
    "    print(\"=== Final Aggregated Results (LaTeX Format) ===\")\n",
    "    df_res = pd.DataFrame(results_dict)\n",
    "    \n",
    "    for col in metric_names:\n",
    "        mean_val = df_res[col].mean()\n",
    "        std_val = df_res[col].std()\n",
    "        print(f\"{col:<15}: ${mean_val:.4f} \\\\pm {std_val:.4f}$\")\n",
    "    print(\"-\" * 60 + \"\\n\")\n",
    "\n",
    "# ==========================================\n",
    "# TASK T4: Credit Card Fraud (Binary)\n",
    "# ==========================================\n",
    "def run_task_t4():\n",
    "    print(f\"\\n{'='*20} STARTING TASK T4 (Credit Card) {'='*20}\")\n",
    "    \n",
    "    # 1. Load Data\n",
    "    print(\"Loading T4 data...\")\n",
    "    df_train = pd.read_csv(PATHS[\"T4_TRAIN\"])\n",
    "    df_test = pd.read_csv(PATHS[\"T4_TEST\"])\n",
    "\n",
    "    # 2. Preprocess\n",
    "    X_train = df_train.drop('Class', axis=1)\n",
    "    y_train = df_train['Class']\n",
    "    X_test = df_test.drop('Class', axis=1)\n",
    "    y_test = df_test['Class']\n",
    "\n",
    "    print(\"Scaling features...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_train_s = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "    X_test_s = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "    # 3. Training Loop\n",
    "    history = {'AUPRC': [], 'F1-score': [], 'Recall': [], 'Latency (ms)': []}\n",
    "    \n",
    "    for seed in SEEDS:\n",
    "        print(f\"Training Seed {seed}...\")\n",
    "        clf = xgb.XGBClassifier(random_state=seed, subsample=0.8, n_jobs=-1)\n",
    "        clf.fit(X_train_s, y_train)\n",
    "        t_start = time.time()\n",
    "        y_prob = clf.predict_proba(X_test_s)[:, 1]\n",
    "        y_pred = (y_prob >= 0.5).astype(int)\n",
    "        t_end = time.time()\n",
    "\n",
    "        total_infer_time = t_end - t_start\n",
    "        latency_ms_per_sample = (total_infer_time * 1000) / X_test_s.shape[0]\n",
    "        history['Latency (ms)'].append(latency_ms_per_sample)\n",
    "\n",
    "        auprc = average_precision_score(y_test, y_prob)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        rec = recall_score(y_test, y_pred)\n",
    "        \n",
    "        history['AUPRC'].append(auprc)\n",
    "        history['F1-score'].append(f1)\n",
    "        history['Recall'].append(rec)\n",
    "        \n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        print(f\"  CM: {cm.tolist()}\")\n",
    "        print(f\"  Latency: {latency_ms_per_sample:.4f} ms/sample\")\n",
    "        print(f\"  [Seed {seed}] AUPRC: {auprc:.4f} | F1-score: {f1:.4f} | Recall: {rec:.4f}\")\n",
    "\n",
    "    print_latex_stats(history, ['AUPRC', 'F1-score', 'Recall'])\n",
    "\n",
    "# ==========================================\n",
    "# TASK T5: UNSW-NB15 (Binary)\n",
    "# ==========================================\n",
    "def run_task_t5():\n",
    "    print(f\"\\n{'='*20} STARTING TASK T5 (UNSW Binary) {'='*20}\")\n",
    "\n",
    "    # 1. Load Data\n",
    "    print(\"Loading T5 data...\")\n",
    "    df_train = pd.read_parquet(PATHS[\"UNSW_TRAIN\"])\n",
    "    df_test = pd.read_parquet(PATHS[\"UNSW_TEST\"])\n",
    "\n",
    "    target_col = 'label'\n",
    "    drop_cols = ['attack_cat'] # Drop multiclass target\n",
    "\n",
    "    X_train = df_train.drop(columns=[target_col] + drop_cols)\n",
    "    y_train = df_train[target_col]\n",
    "    X_test = df_test.drop(columns=[target_col] + drop_cols)\n",
    "    y_test = df_test[target_col]\n",
    "\n",
    "    # 2. Preprocess (OHE + Scaling)\n",
    "    print(\"Encoding & Scaling...\")\n",
    "    X_train = pd.get_dummies(X_train)\n",
    "    X_test = pd.get_dummies(X_test)\n",
    "    X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_s = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "    X_test_s = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "    # 3. Training Loop\n",
    "    history = {'AUPRC': [], 'F1-score': [], 'Recall': [], 'Latency (ms)': []}\n",
    "\n",
    "    for seed in SEEDS:\n",
    "        print(f\"Training Seed {seed}...\")\n",
    "        clf = xgb.XGBClassifier(random_state=seed, subsample=0.8, n_jobs=-1)\n",
    "        clf.fit(X_train_s, y_train)\n",
    "        t_start = time.time()\n",
    "        y_prob = clf.predict_proba(X_test_s)[:, 1]\n",
    "        y_pred = (y_prob >= 0.5).astype(int)\n",
    "        t_end = time.time()\n",
    "\n",
    "        total_infer_time = t_end - t_start\n",
    "        latency_ms_per_sample = (total_infer_time * 1000) / X_test_s.shape[0]\n",
    "        history['Latency (ms)'].append(latency_ms_per_sample)\n",
    "\n",
    "        auprc = average_precision_score(y_test, y_prob)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        rec = recall_score(y_test, y_pred)\n",
    "\n",
    "        history['AUPRC'].append(auprc)\n",
    "        history['F1-score'].append(f1)\n",
    "        history['Recall'].append(rec)\n",
    "\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        print(f\"  CM: {cm.tolist()}\")\n",
    "        print(f\"  Latency: {latency_ms_per_sample:.4f} ms/sample\")\n",
    "        print(f\"  [Seed {seed}] AUPRC: {auprc:.4f} | F1-score: {f1:.4f} | Recall: {rec:.4f}\")\n",
    "\n",
    "    print_latex_stats(history, ['AUPRC', 'F1-score', 'Recall'])\n",
    "\n",
    "# ==========================================\n",
    "# TASK T6: UNSW-NB15 (Multiclass)\n",
    "# ==========================================\n",
    "def run_task_t6():\n",
    "    print(f\"\\n{'='*20} STARTING TASK T6 (UNSW Multiclass) {'='*20}\")\n",
    "\n",
    "    # 1. Load Data\n",
    "    print(\"Loading T6 data...\")\n",
    "    df_train = pd.read_parquet(PATHS[\"UNSW_TRAIN\"])\n",
    "    df_test = pd.read_parquet(PATHS[\"UNSW_TEST\"])\n",
    "\n",
    "    target_col = 'attack_cat'\n",
    "    drop_cols = ['label'] # Drop binary target\n",
    "\n",
    "    X_train = df_train.drop(columns=[target_col] + drop_cols)\n",
    "    y_train = df_train[target_col]\n",
    "    X_test = df_test.drop(columns=[target_col] + drop_cols)\n",
    "    y_test = df_test[target_col]\n",
    "\n",
    "    # 2. Preprocess\n",
    "    print(\"Encoding Targets & Features...\")\n",
    "    le = LabelEncoder()\n",
    "    y_train = le.fit_transform(y_train)\n",
    "    y_test = le.transform(y_test)\n",
    "    classes = le.classes_\n",
    "    \n",
    "    X_train = pd.get_dummies(X_train)\n",
    "    X_test = pd.get_dummies(X_test)\n",
    "    X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n",
    "    print(\"Scaling...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_train_s = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "    X_test_s = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "    # 3. Training Loop\n",
    "    history = {'Macro AUPRC': [], 'Macro F1-score': [], 'Macro Recall': [], 'Latency (ms)': []}\n",
    "\n",
    "    for seed in SEEDS:\n",
    "        print(f\"Training Seed {seed}...\")\n",
    "        clf = xgb.XGBClassifier(random_state=seed, subsample=0.8, n_jobs=-1)\n",
    "        clf.fit(X_train_s, y_train)\n",
    "        t_start = time.time()\n",
    "        y_prob = clf.predict_proba(X_test_s)\n",
    "        y_pred = np.argmax(y_prob, axis=1)\n",
    "        t_end = time.time()\n",
    "\n",
    "        total_infer_time = t_end - t_start\n",
    "        latency_ms_per_sample = (total_infer_time * 1000) / X_test_s.shape[0]\n",
    "        history['Latency (ms)'].append(latency_ms_per_sample)\n",
    "\n",
    "        y_test_bin = label_binarize(y_test, classes=np.arange(len(classes)))\n",
    "        \n",
    "        auprc = average_precision_score(y_test_bin, y_prob, average='macro')\n",
    "        f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "        rec = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "\n",
    "        history['Macro AUPRC'].append(auprc)\n",
    "        history['Macro F1-score'].append(f1)\n",
    "        history['Macro Recall'].append(rec)\n",
    "        \n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        print(f\"  CM: {cm.tolist()}\")\n",
    "        print(f\"  Latency: {latency_ms_per_sample:.4f} ms/sample\")\n",
    "        print(f\"  [Seed {seed}] AUPRC: {auprc:.4f} | F1-score: {f1:.4f} | Recall: {rec:.4f}\")\n",
    "\n",
    "    print_latex_stats(history, ['Macro AUPRC', 'Macro F1-score', 'Macro Recall'])\n",
    "\n",
    "# ==========================================\n",
    "# MAIN EXECUTION\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        run_task_t4()\n",
    "        run_task_t5()\n",
    "        run_task_t6()\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\n[ERROR] File not found. Please check paths in 'PATHS' dict.\\n{e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[ERROR] An error occurred:\\n{e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
