{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ==============================================================================\n",
    "#   USER CONFIGURATION SECTION\n",
    "# ==============================================================================\n",
    "\n",
    "# Select Task: \"T2\", \"T3\", \"T4\", \"T5\", \"T6\"\n",
    "TASK_ID = \"T2\"\n",
    "\n",
    "# --- Default Settings ---\n",
    "OUTPUT_SUFFIX = \"\"         # Default suffix (e.g. filename_list.csv)\n",
    "IGNORED_COLS = []          # Columns to exclude from features (besides label)\n",
    "\n",
    "# --- Task Specific Settings ---\n",
    "\n",
    "if TASK_ID == \"T2\":\n",
    "    # [T2] Malicious URL Binary\n",
    "    INPUT_FILES = [\n",
    "        \"/path/to/train_malicious_phish.csv\",\n",
    "        \"/path/to/test_malicious_phish.csv\"\n",
    "    ]\n",
    "    LABEL_COL = \"label\"\n",
    "\n",
    "elif TASK_ID == \"T3\":\n",
    "    # [T3] Malicious URL Multi\n",
    "    INPUT_FILES = [\n",
    "        \"/path/to/train_malicious_phish_multi.csv\",\n",
    "        \"/path/to/test_malicious_phish_multi.csv\"\n",
    "    ]\n",
    "    LABEL_COL = \"label\"\n",
    "\n",
    "elif TASK_ID == \"T4\":\n",
    "    # [T4] Credit Card Fraud Detection\n",
    "    INPUT_FILES = [\n",
    "        \"/path/to/train_creditcard_timesplit.csv\",\n",
    "        \"/path/to/test_creditcard_timesplit.csv\"\n",
    "    ]\n",
    "    LABEL_COL = \"Class\"\n",
    "\n",
    "elif TASK_ID == \"T5\":\n",
    "    # [T5] UNSW-NB15 Binary Classification\n",
    "    # Target: label (0/1). Ignore: attack_cat.\n",
    "    INPUT_FILES = [\n",
    "        \"/path/to/UNSW_NB15_training-set.parquet\",\n",
    "        \"/path/to/UNSW_NB15_testing-set.parquet\"\n",
    "    ]\n",
    "    LABEL_COL = \"label\"\n",
    "    IGNORED_COLS = [\"attack_cat\"]  # Prevent data leakage or redundancy\n",
    "\n",
    "elif TASK_ID == \"T6\":\n",
    "    # [T6] UNSW-NB15 Multi-class Classification\n",
    "    # Target: attack_cat. Ignore: label.\n",
    "    INPUT_FILES = [\n",
    "        \"/path/to/UNSW_NB15_training-set.parquet\",\n",
    "        \"/path/to/UNSW_NB15_testing-set.parquet\"\n",
    "    ]\n",
    "    LABEL_COL = \"attack_cat\"\n",
    "    IGNORED_COLS = [\"label\"]       # Prevent data leakage\n",
    "    OUTPUT_SUFFIX = \"_multi\"       # Output files will be: filename_multi_list.csv\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Unknown Task ID: {TASK_ID}\")\n",
    "\n",
    "# ==============================================================================\n",
    "#   CORE LOGIC (Generic Serialization)\n",
    "# ==============================================================================\n",
    "\n",
    "def _is_invalid(val) -> bool:\n",
    "    \"\"\"Check if value is NaN or Infinite.\"\"\"\n",
    "    return pd.isna(val) or (isinstance(val, (int, float, np.number)) and np.isinf(val))\n",
    "\n",
    "def load_data(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load CSV or Parquet based on extension.\"\"\"\n",
    "    if path.endswith(\".parquet\"):\n",
    "        return pd.read_parquet(path)\n",
    "    else:\n",
    "        return pd.read_csv(path, low_memory=False)\n",
    "\n",
    "def main():\n",
    "    print(f\"=== Starting Serialization for Task [{TASK_ID}] ===\")\n",
    "    \n",
    "    for file_path in INPUT_FILES:\n",
    "        if not os.path.exists(file_path):\n",
    "            warnings.warn(f\"File not found: {file_path}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing: {os.path.basename(file_path)}\")\n",
    "        \n",
    "        try:\n",
    "            # 1. Load Data\n",
    "            df = load_data(file_path)\n",
    "            \n",
    "            if LABEL_COL not in df.columns:\n",
    "                warnings.warn(f\"Label column '{LABEL_COL}' not found in {file_path}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # 2. Separate Label and Features\n",
    "            y_series = df[LABEL_COL]\n",
    "            \n",
    "            # Drop Label and any specific Ignored Columns\n",
    "            cols_to_drop = [LABEL_COL] + [c for c in IGNORED_COLS if c in df.columns]\n",
    "            X_df = df.drop(columns=cols_to_drop)\n",
    "\n",
    "            # 3. Serialize Features\n",
    "            list_style = []\n",
    "            \n",
    "            for _, row in X_df.iterrows():\n",
    "                parts_list = []\n",
    "                \n",
    "                for col, val in row.items():\n",
    "                    # Skip NaNs or Infs\n",
    "                    if _is_invalid(val):\n",
    "                        continue\n",
    "                    \n",
    "                    # Create string representation (List style only)\n",
    "                    parts_list.append(f\"{col}: {val};\")\n",
    "                \n",
    "                list_style.append(\" \".join(parts_list))\n",
    "\n",
    "            # 4. Construct Output DataFrame\n",
    "            df_list = pd.DataFrame({\"data\": list_style, \"label\": y_series})\n",
    "\n",
    "            # 5. Save Files\n",
    "            base_dir = os.path.dirname(file_path)\n",
    "            base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "            \n",
    "            # Construct filename with optional suffix\n",
    "            out_name_list = f\"{base_name}{OUTPUT_SUFFIX}_list.csv\"\n",
    "            path_list = os.path.join(base_dir, out_name_list)\n",
    "            \n",
    "            df_list.to_csv(path_list, index=False)\n",
    "            \n",
    "            print(f\" -> Saved: {out_name_list}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"Error processing {file_path}: {str(e)}\")\n",
    "\n",
    "    print(\"\\n=== All Files Processed ===\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
